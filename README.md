# NLP_Projects

* **ngrams normalization**:
  * **Implement a bigram and a trigram language model (for word sequences), using Laplace smoothing**
  
  * **Train our models on a training subset of a corpus, after replacing all the rare words of the training subset (e.g., words that do not occur at least 10 times in the training subset) by a special token *rare* **
  
  * **Check the log-probabilities that our trained models return when given (correct) sentences from the test subset vs.(incorrect) sentences of the  same  length  (in  words)  consisting  of  randomly  selected vocabulary words**
  
  * **Predict the next (vocabulary) word, as in a predictive keyboard**
  
  * **Estimate  the  language  cross-entropy  and  perplexity  of  our  models  on the test subset  of  the corpus.**
